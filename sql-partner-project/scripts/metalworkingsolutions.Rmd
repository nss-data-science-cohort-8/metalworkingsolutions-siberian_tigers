---
title: "Using Postgre with R"
output: html_notebook
---

First, we need to establish a connection with our database. For this, we will use the RPostgres and DBI libraries.

If you are working with a large database, you might consider using the dbplyr library (<https://dbplyr.tidyverse.org/articles/dbplyr.html>), which allows you to use dplyr verbs to interact with tables in a database, but actually execute them in the database.

```{r}
library(DBI)
library(RPostgres)
library(tidyverse)
library(plotly)
library(forcats)
options(scipen=999)
```

```{r}
complete_data <- read_csv('../data/completed_table.csv')
```

```{r}
complete_data |> 
  mutate(prod = mean(estimated_production_hours)) |> 
  mutate(rev = mean(estimated_revenue_per_hour)) |> 
  mutate(total = mean(total_revenue)) |> 
  select(prod, rev, total) |> 
  head(1)
```

```{r}
complete_data |>
 arrange(desc(job_id))
```

```{r}
jobs_hours <- complete_data |> 
  group_by(part_id, job_id) |> 
  summarize(jobs = min(number_of_jobs_by_part),
            hours = mean(estimated_production_hours)) |> 
  arrange(desc(job_id))

jobs_hours |> 
  head(5)
```


```{r}
jobs_hours |> 
  ggplot(aes(x=hours, y=jobs)) +
  geom_line()
```



```{r}
rev_hours <- complete_data |> 
  group_by(part_id) |> 
  summarize(hours = mean(estimated_production_hours), revenue = mean(total_revenue)) |> 
  arrange(desc(hours))

rev_hours |> 
  head(5)
```

```{r}
rev_hours |> 
  ggplot(aes(x=hours, y=revenue)) +
  geom_line()
```


```{r}
complete_data <- read_csv('../data/completed_table.csv')

parts_table_top_10 <- complete_data |> 
  group_by(part_id) |> 
  summarize(jobs = min(number_of_jobs_by_part),
            estimated_hours = sum(estimated_production_hours),
            revenue = sum(total_revenue),
            estimated_job_per_hour = (sum(estimated_production_hours) / min(number_of_jobs_by_part)),             estimated_revenue_per_job = sum(total_revenue) / min(number_of_jobs_by_part),
            revenue_per_hour = mean(estimated_revenue_per_hour)) |> 
  arrange(desc(jobs)) |> 
  head(10) 

parts_table_top_10
```


```{r}
sf <- max(parts_table$jobs)/max(parts_table$estimated_hours)

parts_longer <- parts_table_top_10 |> 
  mutate(estimated_hours = estimated_hours*sf) |> 
  pivot_longer(names_to = 'y_new', values_to = 'val', jobs:estimated_hours)

subset_to_order <- parts_longer |> 
  filter(y_new == 'jobs')
subset_to_order$part_id = fct_reorder(subset_to_order$part_id, -subset_to_order$val)
parts_longer$part_id = factor(parts_longer$part_id, levels = levels(subset_to_order$part_id))
  
ggplot(parts_longer, aes(x=part_id)) +
  geom_col(aes(y = val, fill = y_new, group = y_new), position=position_dodge(),
            color="black", alpha=.6)  +
  scale_fill_manual(values = c("red", "blue")) +
  scale_y_continuous(name = "number of jobs",labels = scales::comma,sec.axis = sec_axis(~./sf, name="estimated production hours",
                                                     labels = scales::comma))+
  labs(fill='variable')+
  theme_bw()+
  theme(legend.position = 'top',
        plot.title = element_text(color='black',face='bold',hjust=0.5),
        axis.text = element_text(color='black',face='bold'),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.y.right = element_text(color='red',face='bold'),
        axis.title.y.left = element_text(color='blue',face='bold'),
        legend.text = element_text(color='black',face='bold'),
        legend.title = element_text(color='black',face='bold'))+
  ggtitle('My barplot')
```



```{r}
data_long <- parts_table %>%
  pivot_longer(cols = c("jobs", "estimated_hours"), names_to = "variable", values_to = "value")

# Create the plot with secondary axis
ggplot(data_long, aes(x = part_id, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_continuous(
    sec.axis = sec_axis(~ . * .001, name = "Value 2 (scaled)")
  ) +
  labs(x = "Category", y = "Value 1", fill = "") +
  theme_bw()
```














Now, we'll create our connection.

```{r}
con <- dbConnect(Postgres(),                 # Use the postgres driver
                 dbname = 'metalworkingsolutions',     # Name of the database we want to connect to
                 host = 'localhost',         # 'localhost' or eg. 'ec2-54-83-201-96.compute-1.amazonaws.com'
                 port = 5432, 
                 user = 'postgres',
                 password = rstudioapi::askForPassword("Database password"))
```

We can see the tables in the database.

```{r}
dbListTables(con)
```

We can even see the columns of a particular table.

```{r}
dbListFields(con, 'parts')
```

To execute a query, we can use the dbSendQuery function

```{r}
query = "WITH parts_jobs AS(
SELECT
	imp_short_description
	,jmp_part_id
	,COUNT(jmp_job_id) AS n_of_jobs
FROM jobs a
FULL JOIN parts b
	ON a.jmp_part_id = b.imp_part_id
	GROUP BY 1,2),
job_ops_count AS(
SELECT 
	jmo_job_id
	,SUM(a.jmo_actual_production_hours) AS twenty_four
	,SUM(b.jmo_actual_production_hours) AS twenty_three
	,(SUM(a.jmo_actual_production_hours) + SUM(b.jmo_actual_production_hours)) AS total_hours
FROM job_operations_2024 a
FULL JOIN job_operations_2023 b
	USING(jmo_job_id)
GROUP BY jmo_job_id
ORDER BY twenty_four DESC
),
full_hours AS(
SELECT
	jmo_job_id
	,COALESCE(total_hours, twenty_three, twenty_four) AS hours
FROM job_ops_count
ORDER BY 2 DESC
),
join_cte AS(
SELECT
	jmo_job_id
	,hours
	,b.jmp_part_id
	,c.imp_short_description
	,n_of_jobs
FROM full_hours a
JOIN jobs b
	ON a.jmo_job_id = b.jmp_job_id
JOIN parts c
	ON b.jmp_part_id = c.imp_part_id
JOIN parts_jobs d
	ON c.imp_part_id = d.jmp_part_id
ORDER BY 2 DESC)
SELECT 
	imp_short_description
	,jmp_part_id
	,SUM(hours) AS production_hours
	,n_of_jobs
FROM join_cte
GROUP BY imp_short_description, jmp_part_id, n_of_jobs
ORDER BY 3 DESC;"

res <- dbSendQuery(con, query)
```

Then to retrieve the results, we can use dbFetch. Note that you can fetch only a portion of the results if needed, but we'll fetch everything.

```{r}
parts_production <- dbFetch(res)

# It is good practice to clear the result after fetching what you need in order to free all resources associated with the result set. 
dbClearResult(res)
```

The results that are returned come through as a dataframe, so we can manipulate it using tidyverse.

```{r}
parts_production %>% 
  filter(n_of_jobs > 50)
```
